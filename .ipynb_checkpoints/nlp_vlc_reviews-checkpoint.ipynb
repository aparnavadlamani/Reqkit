{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Collection and Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Noun, verb, and adjective extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#importing natural language tookit\n",
    "\n",
    "import nltk\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "#review of VLC app for andriod in google play\n",
    "text=\"Twice now in the past couple of months, I have had all my TV shows and movies downloaded here and twice within a couple of days most or partial of them are gone? I don't know if this has to do with an update, but whatever it is it's not working. Frustrating having to go back and put them in again......\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenizing words from review1\n",
    "text1=nltk.word_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Twice', 'RB'),\n",
       " ('now', 'RB'),\n",
       " ('in', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('past', 'JJ'),\n",
       " ('couple', 'NN'),\n",
       " ('of', 'IN'),\n",
       " ('months', 'NNS'),\n",
       " (',', ','),\n",
       " ('I', 'PRP'),\n",
       " ('have', 'VBP'),\n",
       " ('had', 'VBN'),\n",
       " ('all', 'DT'),\n",
       " ('my', 'PRP$'),\n",
       " ('TV', 'NN'),\n",
       " ('shows', 'NNS'),\n",
       " ('and', 'CC'),\n",
       " ('movies', 'NNS'),\n",
       " ('downloaded', 'VBN'),\n",
       " ('here', 'RB'),\n",
       " ('and', 'CC'),\n",
       " ('twice', 'RB'),\n",
       " ('within', 'IN'),\n",
       " ('a', 'DT'),\n",
       " ('couple', 'NN'),\n",
       " ('of', 'IN'),\n",
       " ('days', 'NNS'),\n",
       " ('most', 'RBS'),\n",
       " ('or', 'CC'),\n",
       " ('partial', 'JJ'),\n",
       " ('of', 'IN'),\n",
       " ('them', 'PRP'),\n",
       " ('are', 'VBP'),\n",
       " ('gone', 'VBN'),\n",
       " ('?', '.'),\n",
       " ('I', 'PRP'),\n",
       " ('do', 'VBP'),\n",
       " (\"n't\", 'RB'),\n",
       " ('know', 'VB'),\n",
       " ('if', 'IN'),\n",
       " ('this', 'DT'),\n",
       " ('has', 'VBZ'),\n",
       " ('to', 'TO'),\n",
       " ('do', 'VB'),\n",
       " ('with', 'IN'),\n",
       " ('an', 'DT'),\n",
       " ('update', 'NN'),\n",
       " (',', ','),\n",
       " ('but', 'CC'),\n",
       " ('whatever', 'IN'),\n",
       " ('it', 'PRP'),\n",
       " ('is', 'VBZ'),\n",
       " ('it', 'PRP'),\n",
       " (\"'s\", 'VBZ'),\n",
       " ('not', 'RB'),\n",
       " ('working', 'VBG'),\n",
       " ('.', '.'),\n",
       " ('Frustrating', 'VBG'),\n",
       " ('having', 'VBG'),\n",
       " ('to', 'TO'),\n",
       " ('go', 'VB'),\n",
       " ('back', 'RB'),\n",
       " ('and', 'CC'),\n",
       " ('put', 'VB'),\n",
       " ('them', 'PRP'),\n",
       " ('in', 'IN'),\n",
       " ('again', 'RB'),\n",
       " ('...', ':'),\n",
       " ('...', ':')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#using part of speech (POS) tagging functionality of the Natural Language Toolkit \n",
    "#identifying and extracting the nouns, verbs, and adjectives in the reviews.\n",
    "\n",
    "nltk.pos_tag(text1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenizing words\n",
    "\n",
    "text1=nltk.word_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Twice', 'now', 'in', 'the', 'past', 'couple', 'of', 'months', ',', 'I', 'have', 'had', 'all', 'my', 'TV', 'shows', 'and', 'movies', 'downloaded', 'here', 'and', 'twice', 'within', 'a', 'couple', 'of', 'days', 'most', 'or', 'partial', 'of', 'them', 'are', 'gone', '?', 'I', 'do', \"n't\", 'know', 'if', 'this', 'has', 'to', 'do', 'with', 'an', 'update', ',', 'but', 'whatever', 'it', 'is', 'it', \"'s\", 'not', 'working', '.', 'Frustrating', 'having', 'to', 'go', 'back', 'and', 'put', 'them', 'in', 'again', '...', '...']\n"
     ]
    }
   ],
   "source": [
    "print(text1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stopword removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imporing stopwords from nltkcorpus\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "stop=stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be']"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#standard list of stopwords provided by Lucene\n",
    "\n",
    "stop[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove stopwords to eliminate terms that are very common in the English language\n",
    "\n",
    "text2=[word for word in text1 if word not in stop]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Twice', 'past', 'couple', 'months', ',', 'I', 'TV', 'shows', 'movies', 'downloaded', 'twice', 'within', 'couple', 'days', 'partial', 'gone', '?', 'I', \"n't\", 'know', 'update', ',', 'whatever', \"'s\", 'working', '.', 'Frustrating', 'go', 'back', 'put', '...', '...']\n"
     ]
    }
   ],
   "source": [
    "print(text2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lemmatizer from NLTK for grouping the different inflected forms of words with the same part of speech tag which are syntactically different but semantically equal\n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "porter = PorterStemmer()\n",
    "lancaster=LancasterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word                Porter Stemmer      lancaster Stemmer   \n",
      "Twice               twice               twic                \n",
      "past                past                past                \n",
      "couple              coupl               coupl               \n",
      "months              month               month               \n",
      ",                   ,                   ,                   \n",
      "I                   I                   i                   \n",
      "TV                  TV                  tv                  \n",
      "shows               show                show                \n",
      "movies              movi                movy                \n",
      "downloaded          download            download            \n",
      "twice               twice               twic                \n",
      "within              within              within              \n",
      "couple              coupl               coupl               \n",
      "days                day                 day                 \n",
      "partial             partial             part                \n",
      "gone                gone                gon                 \n",
      "?                   ?                   ?                   \n",
      "I                   I                   i                   \n",
      "n't                 n't                 n't                 \n",
      "know                know                know                \n",
      "update              updat               upd                 \n",
      ",                   ,                   ,                   \n",
      "whatever            whatev              whatev              \n",
      "'s                  's                  's                  \n",
      "working             work                work                \n",
      ".                   .                   .                   \n",
      "Frustrating         frustrat            frust               \n",
      "go                  go                  go                  \n",
      "back                back                back                \n",
      "put                 put                 put                 \n",
      "...                 ...                 ...                 \n",
      "...                 ...                 ...                 \n"
     ]
    }
   ],
   "source": [
    "print(\"{0:20}{1:20}{2:20}\".format(\"Word\",\"Porter Stemmer\",\"lancaster Stemmer\"))\n",
    "for word in text2:\n",
    "    print(\"{0:20}{1:20}{2:20}\".format(word,porter.stem(word),lancaster.stem(word)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### collocation finding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#collocation finding algorithm provided by the NLTK toolkit for extracting features from the user reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.collocations import *\n",
    "from nltk.corpus import webtext\n",
    "from nltk.collocations import BigramCollocationFinder \n",
    "from nltk.metrics import BigramAssocMeasures \n",
    "from nltk.collocations import TrigramCollocationFinder \n",
    "from nltk.metrics import TrigramAssocMeasures "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "trigram_measures = nltk.collocations.TrigramAssocMeasures()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = [w.lower() for w in webtext.words( '/home/keerthana/Desktop/review1')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('couple', 'of'),\n",
       " ('.', 'frustrating'),\n",
       " ('again', '......'),\n",
       " ('all', 'my'),\n",
       " ('an', 'update'),\n",
       " ('are', 'gone'),\n",
       " ('but', 'whatever'),\n",
       " ('days', 'most'),\n",
       " ('do', 'with'),\n",
       " ('downloaded', 'here'),\n",
       " ('frustrating', 'having'),\n",
       " ('go', 'back'),\n",
       " ('gone', '?'),\n",
       " ('had', 'all'),\n",
       " ('have', 'had')]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "biagram_collocation = BigramCollocationFinder.from_words(words) \n",
    "biagram_collocation.nbest(BigramAssocMeasures.likelihood_ratio, 15) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = [w.lower() for w in webtext.words( '/home/keerthana/Desktop/review1')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', 'couple', 'of'),\n",
       " ('couple', 'of', 'days'),\n",
       " ('couple', 'of', 'months'),\n",
       " ('past', 'couple', 'of'),\n",
       " ('.', 'frustrating', 'having'),\n",
       " ('all', 'my', 'tv'),\n",
       " ('are', 'gone', '?'),\n",
       " ('days', 'most', 'or'),\n",
       " ('do', 'with', 'an'),\n",
       " ('had', 'all', 'my'),\n",
       " ('have', 'had', 'all'),\n",
       " ('if', 'this', 'has'),\n",
       " ('know', 'if', 'this'),\n",
       " ('most', 'or', 'partial'),\n",
       " ('movies', 'downloaded', 'here')]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "triagram_collocation = TrigramCollocationFinder.from_words(words) \n",
    "triagram_collocation.nbest(TrigramAssocMeasures.likelihood_ratio, 15) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('movies', 'downloaded'), ('past', 'couple'), ('twice', 'within')]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopset = set(stopwords.words('english')) \n",
    "filter_stops = lambda w: len(w) < 3 or w in stopset \n",
    "biagram_collocation.apply_word_filter(filter_stops) \n",
    "biagram_collocation.nbest(BigramAssocMeasures.likelihood_ratio, 15) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trigram_collocation = TrigramCollocationFinder.from_words(words) \n",
    "trigram_collocation.apply_word_filter(filter_stops) \n",
    "trigram_collocation.apply_freq_filter(3) \n",
    "trigram_collocation.nbest(TrigramAssocMeasures.likelihood_ratio, 15) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
